{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b100a972",
   "metadata": {},
   "source": [
    "## **DECISION TREE CLASSIFIER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f5392",
   "metadata": {},
   "source": [
    "In the simple form we can say that decison trees are the nested if else statements.\\\n",
    "**Where is the tree in the Decision tree?**\n",
    "- We form the tree like structure from the if else statements. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206623c",
   "metadata": {},
   "source": [
    "**What if we have numerical data?**\n",
    "- For example in the iris datasets, we create classes of the numerical data. Then we divide the data accrodingly creating the tree. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64542ee4",
   "metadata": {},
   "source": [
    "### **GEOMETRIC INTUITION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424c3b1c",
   "metadata": {},
   "source": [
    "![image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95ba5c",
   "metadata": {},
   "source": [
    "### **Pseudo code**\n",
    "- Begin with the training dataset, whcih should have some feature variables and classification or regresssion output. \n",
    "- Determine the **Best Feature** in the dataset to split the data on; more on how we define **Best Feature** later.\n",
    "- Split the data into the subsets that contain the correct value for this best feature. This splitting basically defines a node on the tree i.e each node is a splitting point based on a certain feature from out data.\n",
    "- Recursively generate new tree nodes by using the subset of data created from step 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d5433",
   "metadata": {},
   "source": [
    "### **Tree stucture**\n",
    "- The base decesion factor is root node.\n",
    "- Splitting point\n",
    "- The nodes in middle are called Decision node\n",
    "- The bottom classifying nodes are the Leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410edc5",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "- Intuitive and easy to understand.\n",
    "- Minimal data preperation is required.\n",
    "- The cost of using the tree for inference is **logarithmic** in the number of data points used to train the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218ab04",
   "metadata": {},
   "source": [
    "### **Disadvantages**\n",
    "- Overfitting \n",
    "- Prone to error for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71307a",
   "metadata": {},
   "source": [
    "### **CART-Classification and Regression Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0760e0a",
   "metadata": {},
   "source": [
    "### **DECISON TREES ENTROPY**\n",
    "In the most layman terms, Entropy is nothing but the measure of disorder. Or you cna also call it the measure of purity/impurity. Let's see an example.  \n",
    "- More knowledge less entropy in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c4e3f",
   "metadata": {},
   "source": [
    "### **How to calculate Entropy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d8e7f",
   "metadata": {},
   "source": [
    "The mathematical formula for Entropy is:\n",
    "- $E(S)=\\sum_{i=1}^c(-p_ilog_2p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb162c",
   "metadata": {},
   "source": [
    "Where 'Pi' is simply the frequentist probablity of an element/class 'i' in our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7053a",
   "metadata": {},
   "source": [
    "### **OBSERVATION**\n",
    "- More the uncertanity more is entropy.\n",
    "- For a 2 class problem the min entropy is 0 and the max is 1.\n",
    "- For more than 2 classes the min entropy is zero but the max can be greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb97f0",
   "metadata": {},
   "source": [
    "### **Entropy vs Probablity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8158c",
   "metadata": {},
   "source": [
    "- If We have only one class then we have the entropy zero and if we have equality in both the calsses then we will have the maximum entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72250287",
   "metadata": {},
   "source": [
    "### **ENTROPY FOR CONTNIOUS VARIABLES**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7c905",
   "metadata": {},
   "source": [
    "- The dataset which have the smaller peak in the kde plot or the standard deviation lesser the entropy will be lesser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb5a03a",
   "metadata": {},
   "source": [
    "### **INFORMATION GAIN**\n",
    "- Information gain, is a metric used to train Decision Trees. Specifically, this metric measures the quality of a split.\n",
    "- The information gain is based on the decrease in entropy after a data-set is split on an attribute. Constructing a decision tree is all about finding attribute that returns the highest information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788ffb4",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{information Gain}=E(Parent)-{\\text{Weighted Average}}*E{\\text{Children}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511afff7",
   "metadata": {},
   "source": [
    "### **Steps to calculate the information gain**\n",
    "1. Calculate the Entropy of Parent.\n",
    "2. Calculate the Entropy for Children(Children being the splitted datasets of the main datasets).\n",
    "3. Calculate weighted Entropy of Children.\n",
    "4. Calcuale the information Gain\n",
    "5. Calculate Information Gain for all the columns\n",
    "6. Find Information Gain recursively.(Decision tree then applies a recursively greedy search algortihm in top bottom fashion to find information Gain at every level of the tree. Once a leaf node is reached(Entropy=0), no more splitting is done.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844ed5d",
   "metadata": {},
   "source": [
    "### **GINI IMPURITY**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2193dcbd",
   "metadata": {},
   "source": [
    "$$ \n",
    "G=1-(P_Y^2+P_N^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c0c24",
   "metadata": {},
   "source": [
    "- lesser the gini impurity, higher the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27595e15",
   "metadata": {},
   "source": [
    "### **WHAT IS THE DIFFERENCE BETWEEN ENTROPY AND GINI IMPURITY?**\n",
    "- For two classes, the max entropy is 0.5. It is used because it is compuatationally fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0facca",
   "metadata": {},
   "source": [
    "### **FOR THE NUMERICAL DATA**\n",
    "1. Sort the numerical data on the basic of numrical column.\n",
    "2. Split the entire data on the basic of every value of the numrical column. We will have the multiple splits.\n",
    "3. Calculate the entropy for each of the child of the each datasets. And for each datasets we calculate the weighted entropy\n",
    "4. Find the information gain of each\n",
    "5. Select the maximum information gain. \n",
    "6. Take the largest information gain datasets as the splitting criteria.\n",
    "7. Recursively do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4199b6d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
