{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4244f26",
   "metadata": {},
   "source": [
    "### **PRECSION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd8104",
   "metadata": {},
   "source": [
    "![image](3.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244572a",
   "metadata": {},
   "source": [
    "Here in the picuter, accuracy of the model is 0.85, which means that model is correct 85% of the time. For the another model too, accuracy is also 85%. Here false postive of A is > false postive of B and false negetive of A is < false negative of B.\\\n",
    "In this example more dangerous would be false postive meaning predicting the email spam when it is not. Thus we will select the model which has less false positive. That is we will use precision to compare the models.\\\n",
    "The meaning of the precision is the proportion of correctly predicted positives (true positives) to the total predicted positives (true positives + false positives).\\\n",
    "In our example above it would be the proportion of no of mails correctly classified as spam to the no of predicted spams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7306ba8",
   "metadata": {},
   "source": [
    "$$\n",
    "precision =\\frac{TP}{TP+FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88d368",
   "metadata": {},
   "source": [
    "### **RECALL**\n",
    "![image](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a067256",
   "metadata": {},
   "source": [
    "Here in the picture, accuracy of the model is 0.90, which means that model is correct 90% of the time. For the another model too accuracy is also 90%.\n",
    "In the above, if we donot detect cancer for the patient who has cancer it would be more dangerous than to detect cancer for the non cancerous patient.\n",
    "Thus we will use proportion of the correctly predicted postives to compare the above models among the total real postives.\n",
    "Recall is used when we are more concerned with the rate of false negatives than the false positives, as in the hospital data. Recall measures the proportion of correctly identified actual positives (true positives) out of all actual positives (true positives + false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce8046",
   "metadata": {},
   "source": [
    "$$\n",
    "Recall=\\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4723b",
   "metadata": {},
   "source": [
    "### **F1-SCORE**\n",
    "The F1-score is a metric that combines both precision and recall into a single value by taking their harmonic mean. It is especially useful when you want to balance the trade-off between precision and recall, and when you have an uneven class distribution.\n",
    "\n",
    "In situations where both false positives and false negatives are important, the F1-score provides a better measure of a model’s performance than accuracy alone. For example, in cases where you want to ensure that not only are your positive predictions correct (high precision), but also that you are capturing as many actual positives as possible (high recall), the F1-score gives a single metric that reflects both concerns.\n",
    "\n",
    "The F1-score is particularly helpful when you need to find an optimal balance between precision and recall, such as in medical diagnostics or spam detection, where both types of errors can have significant consequences.\n",
    "\n",
    "The F1-score is calculated as follows:\n",
    "\n",
    "$$ F1 = 2 \\times \\frac{precision \\times recall}{precision + recall} $$\n",
    "\n",
    "It ranges from 0 to 1, where a higher F1-score indicates better model performance in terms of both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46788439",
   "metadata": {},
   "source": [
    "Using the harmonic mean we obtain the value closer to the lower value. That means the F1 score will be toward precision or recall which is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47c3dc",
   "metadata": {},
   "source": [
    "### **CODE DEMONSTRATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "763f2033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import recall_score,precision_score,f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b6b1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e958db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('C:/Users/Acer/OneDrive/Documents/MACLEARNING/machinelearning/data/advertising.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ad6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.rename(columns={'Clicked on Ad':'click','Daily Time Spent on Site':'time','Age':'age'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807c00b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>age</th>\n",
       "      <th>Area Income</th>\n",
       "      <th>Daily Internet Usage</th>\n",
       "      <th>Ad Topic Line</th>\n",
       "      <th>City</th>\n",
       "      <th>Male</th>\n",
       "      <th>Country</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.95</td>\n",
       "      <td>35</td>\n",
       "      <td>61833.90</td>\n",
       "      <td>256.09</td>\n",
       "      <td>Cloned 5thgeneration orchestration</td>\n",
       "      <td>Wrightburgh</td>\n",
       "      <td>0</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>2016-03-27 00:53:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.23</td>\n",
       "      <td>31</td>\n",
       "      <td>68441.85</td>\n",
       "      <td>193.77</td>\n",
       "      <td>Monitored national standardization</td>\n",
       "      <td>West Jodi</td>\n",
       "      <td>1</td>\n",
       "      <td>Nauru</td>\n",
       "      <td>2016-04-04 01:39:02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.47</td>\n",
       "      <td>26</td>\n",
       "      <td>59785.94</td>\n",
       "      <td>236.50</td>\n",
       "      <td>Organic bottom-line service-desk</td>\n",
       "      <td>Davidton</td>\n",
       "      <td>0</td>\n",
       "      <td>San Marino</td>\n",
       "      <td>2016-03-13 20:35:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.15</td>\n",
       "      <td>29</td>\n",
       "      <td>54806.18</td>\n",
       "      <td>245.89</td>\n",
       "      <td>Triple-buffered reciprocal time-frame</td>\n",
       "      <td>West Terrifurt</td>\n",
       "      <td>1</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2016-01-10 02:31:19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.37</td>\n",
       "      <td>35</td>\n",
       "      <td>73889.99</td>\n",
       "      <td>225.58</td>\n",
       "      <td>Robust logistical utilization</td>\n",
       "      <td>South Manuel</td>\n",
       "      <td>0</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>2016-06-03 03:36:18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    time  age  Area Income  Daily Internet Usage  \\\n",
       "0  68.95   35     61833.90                256.09   \n",
       "1  80.23   31     68441.85                193.77   \n",
       "2  69.47   26     59785.94                236.50   \n",
       "3  74.15   29     54806.18                245.89   \n",
       "4  68.37   35     73889.99                225.58   \n",
       "\n",
       "                           Ad Topic Line            City  Male     Country  \\\n",
       "0     Cloned 5thgeneration orchestration     Wrightburgh     0     Tunisia   \n",
       "1     Monitored national standardization       West Jodi     1       Nauru   \n",
       "2       Organic bottom-line service-desk        Davidton     0  San Marino   \n",
       "3  Triple-buffered reciprocal time-frame  West Terrifurt     1       Italy   \n",
       "4          Robust logistical utilization    South Manuel     0     Iceland   \n",
       "\n",
       "             Timestamp  click  \n",
       "0  2016-03-27 00:53:11      0  \n",
       "1  2016-04-04 01:39:02      0  \n",
       "2  2016-03-13 20:35:42      0  \n",
       "3  2016-01-10 02:31:19      0  \n",
       "4  2016-06-03 03:36:18      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6b628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[['time','age']]\n",
    "y=data['click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc41fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b982fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=43,test_size=0.3,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b784cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6530cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "log=LogisticRegression()\n",
    "log.fit(X_train,y_train)\n",
    "y_pred1=log.predict(X_test)\n",
    "tree=DecisionTreeClassifier()\n",
    "tree.fit(X_train,y_train)\n",
    "y_pred2=tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1338f865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for logititc regression\n",
      "------------------------------------------------------------\n",
      "     0    1\n",
      "0  136   14\n",
      "1   26  124\n",
      "precision: 0.8985507246376812\n",
      "recall: 0.8266666666666667\n",
      "f1 score: 0.8611111111111112\n",
      "\n",
      "for decision tree\n",
      "------------------------------------------------------------\n",
      "     0    1\n",
      "0  134   16\n",
      "1   31  119\n",
      "precision: 0.8814814814814815\n",
      "recall: 0.7933333333333333\n",
      "f1 score: 0.8350877192982457\n"
     ]
    }
   ],
   "source": [
    "print(\"for logititc regression\")\n",
    "print(\"-\"*60)\n",
    "cdf=pd.DataFrame(confusion_matrix(y_test,y_pred1),columns=list(range(0,2)))\n",
    "print(cdf)\n",
    "print('precision:',precision_score(y_test,y_pred1))\n",
    "print('recall:',recall_score(y_test,y_pred1))\n",
    "print('f1 score:',f1_score(y_test,y_pred1))\n",
    "print(\"\\nfor decision tree\")\n",
    "print(\"-\"*60)\n",
    "cdf=pd.DataFrame(confusion_matrix(y_test,y_pred2),columns=list(range(0,2)))\n",
    "print(cdf)\n",
    "print('precision:',precision_score(y_test,y_pred2))     \n",
    "print('recall:',recall_score(y_test,y_pred2))\n",
    "print('f1 score:',f1_score(y_test,y_pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a226cc",
   "metadata": {},
   "source": [
    "### **MULTI CLASS PRECISON AND RECALL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a60ca",
   "metadata": {},
   "source": [
    "### Multiclass Precision, Recall, and F1-Score\n",
    "\n",
    "In multiclass classification, precision, recall, and F1-score are calculated for each class individually, treating each class as the \"positive\" class and the rest as \"negative.\" To summarize performance across all classes, we use averaging methods such as **macro** and **weighted**.\n",
    "\n",
    "#### **Macro Averaging**\n",
    "- **Macro** calculates the metric independently for each class and then takes the average (unweighted mean).\n",
    "- Each class is treated equally, regardless of how many samples it has.\n",
    "- Useful when all classes are equally important.\n",
    "\n",
    "#### **Weighted Averaging**\n",
    "- **Weighted** calculates the metric for each class, then takes the average weighted by the number of true instances for each class (support).\n",
    "- Classes with more samples have a greater impact on the final score.\n",
    "- Useful when class imbalance is present.\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "Suppose you have a 3-class classification problem (classes: A, B, C) and the following confusion matrix:\n",
    "\n",
    "|        | Pred A | Pred B | Pred C |\n",
    "|--------|--------|--------|--------|\n",
    "| **A**  |   40   |   2    |   3    |\n",
    "| **B**  |   4    |   35   |   1    |\n",
    "| **C**  |   6    |   2    |   30   |\n",
    "\n",
    "- **Precision for A** = TP / (TP + FP) = 40 / (40 + 4 + 6) = 40 / 50 = 0.80  \n",
    "- **Recall for A** = TP / (TP + FN) = 40 / (40 + 2 + 3) = 40 / 45 ≈ 0.89  \n",
    "- **F1 for A** = 2 × (Precision × Recall) / (Precision + Recall) ≈ 0.84\n",
    "\n",
    "Repeat for B and C.\n",
    "\n",
    "##### **Macro Average**\n",
    "- Macro Precision = (Precision_A + Precision_B + Precision_C) / 3\n",
    "- Macro Recall = (Recall_A + Recall_B + Recall_C) / 3\n",
    "- Macro F1 = (F1_A + F1_B + F1_C) / 3\n",
    "\n",
    "##### **Weighted Average**\n",
    "- Weighted Precision = (Precision_A × Support_A + Precision_B × Support_B + Precision_C × Support_C) / Total Samples\n",
    "- Weighted Recall and F1 are calculated similarly.\n",
    "\n",
    "#### **In scikit-learn**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# y_true and y_pred are your true and predicted labels\n",
    "precision_macro = precision_score(y_true, y_pred, average='macro')\n",
    "recall_macro = recall_score(y_true, y_pred, average='macro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "```\n",
    "\n",
    "- Use `average='macro'` for macro average.\n",
    "- Use `average='weighted'` for weighted average.\n",
    "\n",
    "**Summary:**  \n",
    "- **Macro**: Equal weight to each class.  \n",
    "- **Weighted**: Weight by class frequency.  \n",
    "- Choose based on whether you want to treat all classes equally or account for class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59edb3aa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
