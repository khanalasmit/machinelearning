{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4244f26",
   "metadata": {},
   "source": [
    "### **PRECSION**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd8104",
   "metadata": {},
   "source": [
    "![image](3.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f244572a",
   "metadata": {},
   "source": [
    "Here in the picuter, accuracy of the model is 0.85, which means that model is correct 85% of the time. For the another model too, accuracy is also 85%. Here false postive of A is > false postive of B and false negetive of A is < false negative of B.\\\n",
    "In this example more dangerous would be false postive meaning predicting the email spam when it is not. Thus we will select the model which has less false positive. That is we will use precision to compare the models.\\\n",
    "The meaning of the precision is the proportion of correctly predicted positives (true positives) to the total predicted positives (true positives + false positives).\\\n",
    "In our example above it would be the proportion of no of mails correctly classified as spam to the no of predicted spams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7306ba8",
   "metadata": {},
   "source": [
    "$$\n",
    "precision =\\frac{TP}{TP+FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88d368",
   "metadata": {},
   "source": [
    "### **RECALL**\n",
    "![image](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a067256",
   "metadata": {},
   "source": [
    "Here in the picture, accuracy of the model is 0.90, which means that model is correct 90% of the time. For the another model too accuracy is also 90%.\n",
    "In the above, if we donot detect cancer for the patient who has cancer it would be more dangerous than to detect cancer for the non cancerous patient.\n",
    "Thus we will use proportion of the correctly predicted postives to compare the above models among the total real postives.\n",
    "Recall is used when we are more concerned with the rate of false negatives than the false positives, as in the hospital data. Recall measures the proportion of correctly identified actual positives (true positives) out of all actual positives (true positives + false negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce8046",
   "metadata": {},
   "source": [
    "$$\n",
    "Recall=\\frac{TP}{TP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4723b",
   "metadata": {},
   "source": [
    "### **F1-SCORE**\n",
    "The F1-score is a metric that combines both precision and recall into a single value by taking their harmonic mean. It is especially useful when you want to balance the trade-off between precision and recall, and when you have an uneven class distribution.\n",
    "\n",
    "In situations where both false positives and false negatives are important, the F1-score provides a better measure of a modelâ€™s performance than accuracy alone. For example, in cases where you want to ensure that not only are your positive predictions correct (high precision), but also that you are capturing as many actual positives as possible (high recall), the F1-score gives a single metric that reflects both concerns.\n",
    "\n",
    "The F1-score is particularly helpful when you need to find an optimal balance between precision and recall, such as in medical diagnostics or spam detection, where both types of errors can have significant consequences.\n",
    "\n",
    "The F1-score is calculated as follows:\n",
    "\n",
    "$$ F1 = 2 \\times \\frac{precision \\times recall}{precision + recall} $$\n",
    "\n",
    "It ranges from 0 to 1, where a higher F1-score indicates better model performance in terms of both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46788439",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
