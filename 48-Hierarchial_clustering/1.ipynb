{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0568dc9e",
   "metadata": {},
   "source": [
    "### **HIERARCHIAL CLUSTERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872bb0f",
   "metadata": {},
   "source": [
    "#### **Need of Other Clustering Methods Than KMeans**\n",
    "- If the datasets have well defined boundaries then the **Kmean** clustering is fully applicalble. This realy upon the distace does not work properly beacuse the clusters with no defined boundary **Kmean** Fails.\n",
    "- Does not ignore the outliers or noises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94649e73",
   "metadata": {},
   "source": [
    "## **TYPES OF Hierarchial Clustering**\n",
    "- Agglomerative clustering\n",
    "- Divisive clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27040f71",
   "metadata": {},
   "source": [
    "### **Agglomerative Clustering**\n",
    "- We start with the dataset and make each individual points a cluster. \n",
    "- Then the closer points are merged in the single cluster and keep the record somewhere. \n",
    "- At the end we will have the single cluster. \n",
    "- We obtain the tree like structure in the record of the cluster which we call as **Dendogram**.\n",
    "- Here we have Hierarchy of the clusters thus it is called Hierarchial cluster. \n",
    "- To obtain the multiple clusters we reduce the dendogram from the root. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e583f6",
   "metadata": {},
   "source": [
    "### **Divisive clustering**\n",
    "- Here it works same as the agglomeritive clustering but in the reverse i.e it starts with the single cluster then try to divide the cluster thus it will break down in the clustering leading to each point being a single cluster. \n",
    "- It is the exact oppostie of the Agglomerative clustering. \n",
    "- It is used less often than other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c59f2",
   "metadata": {},
   "source": [
    "### **Algorithm**\n",
    "- Initalize the proximity matrix.\\\n",
    "&darr;\n",
    "- Make each point a cluster.\\\n",
    "&darr;\n",
    "- Inside a loop\n",
    "    * Merge the 2 closet cluster.\n",
    "    * Update the Proximity matrix\\\n",
    "&darr;\n",
    "- Until only one cluster is left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10651e",
   "metadata": {},
   "source": [
    "The distance between the clusters is found by:\n",
    "- Taking distance between clostet points. OR\n",
    "- Taking the distance between the means.\n",
    "- Thus there are many types "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfddf7",
   "metadata": {},
   "source": [
    "### **TYPES OF Agglomeratice Clustering**(linkage)\n",
    "1. Min(Single-link)\n",
    "2. Max(Complete Link)\n",
    "3. Average\n",
    "4. Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a666782",
   "metadata": {},
   "source": [
    "1. Single-Link\n",
    "- We find the distace of each points in one cluster to the each points of the other cluster and find the minimum distace and find the similarity.\n",
    "- If there are outliers in the data the logic fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bf0f6",
   "metadata": {},
   "source": [
    "2. Complete link\n",
    "- First distances as the single link are calculated then we do the clustering based on the max distance.\n",
    "- It helps in the outlier control.\n",
    "- It is not desirable for the large cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4a81b",
   "metadata": {},
   "source": [
    "3. Group Average\n",
    "- We calculate the distance between two clusters by taking averge of the distance of each of the combination between clustess.\n",
    "- It lies between min and max thus effect also in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98495171",
   "metadata": {},
   "source": [
    "4. Ward\n",
    "- We take a centroid for every clusters and then take distance from centroids to the every points. We then sum square of the distances and substract the square of the distances of the mean point with in the cluster to the every point with in cluster for each of the clusters.\n",
    "- *It is the deafult for the sklearn.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e006cf3",
   "metadata": {},
   "source": [
    "### **Ideal Number of the clusters.**\n",
    "- Here we donot know the optimal number of the clusters thus we tkae the help of the **dendogram**. \n",
    "- We need to the longest vertical line such that the horizontal line is not cut by the vertical line. \n",
    "- Thus we cut that vertical lines by horizontal line thus we get the number of teh clusters.\\\n",
    "![image](1.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
