{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8567e2bf",
   "metadata": {},
   "source": [
    "### **PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a6310",
   "metadata": {},
   "source": [
    "**1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbd3972",
   "metadata": {},
   "source": [
    "It stands for principal component analysis. It is used in dimensionality reduction. It is the technique of the feature extraction.\n",
    "PCA finds the best possible lower dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80955485",
   "metadata": {},
   "source": [
    "**Benefits**\n",
    "* With the low number of features in data, faster execution of the algorithm.\n",
    "* Visulaization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900112e",
   "metadata": {},
   "source": [
    "**2. Geometric Intution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6accbf61",
   "metadata": {},
   "source": [
    "A sample data\n",
    "|No of rooms|No of grocery shops|Price|\n",
    "|------|--------|-------|\n",
    "|3|2|60|\n",
    "|4|0|130|\n",
    "|5|6|170|\n",
    "|2|10|90|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e649d93",
   "metadata": {},
   "source": [
    "If we want to predict the price of the house, no of rooms in the features would be more necessay feature than the no of grocery shop around neighbours. This is feature selection. \n",
    "The proper mathematical way is using pca. Suppose above data is plotted in the scatter plot. Then they are projected in the co-ordinated axes. One with the largest distance in coordinate axes will be selcted and other is discared.\n",
    "![image](pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99490089",
   "metadata": {},
   "source": [
    "Above we determined the no of groceries were not much of significance but lets say we got some varience. Which is feature selection. But feature extraction is something different.\\\n",
    "for the following data\n",
    "|No of rooms|No of washrooms|Price|\n",
    "|------|--------|-------|\n",
    "|3|2|60|\n",
    "|4|0|130|\n",
    "|5|6|170|\n",
    "|2|10|90|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab11ca",
   "metadata": {},
   "source": [
    "![image](pca2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87123d3",
   "metadata": {},
   "source": [
    "**3.Why is varience important?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cb39c3",
   "metadata": {},
   "source": [
    "We may have the mutliple datas with same mean. So with mean we may not be able to difference in the data. So we need varience to show difference in two data.\n",
    "Vareience can be given as:\\\n",
    "$\\text{Variance: } \\sum_{i=1}^{n} \\left( \\frac{X_i - \\overline{X}}{n} \\right)^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab1bab0",
   "metadata": {},
   "source": [
    "When we plot the data from higher dimension to the lower dimension we need varience to represent actual distacnce in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e8ba6",
   "metadata": {},
   "source": [
    "**4. Problem formulation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eada30",
   "metadata": {},
   "source": [
    "Here we want to maximize the projection of the point so the scatter plot on unit $\\vec{u}$ which is to maximize the varience on that line.\\\n",
    "i.e\\\n",
    "$\\frac{\\sum_{i=1}^{n}\\left(u^{T}x_i-u^{T}\\overline{x}\\right)^2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e0a99",
   "metadata": {},
   "source": [
    "So we need to find the vector $\\vec{u}$ for which the above term will be maximum. \\\n",
    "Now we will optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f9988",
   "metadata": {},
   "source": [
    "**Covarience and Covarience Matrix**\n",
    "Vareience doesnot tell the relation between two features. For the mutiple datasets varience may be same although for the different direction of spread in the datasets. \\\n",
    "Here in this covarience is needed.\\\n",
    "$\\text{covarience}=\\sum_{i=1}^{n}\\left (\\frac{(X-\\overline{X})*(Y-\\overline{Y})}{N}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79bd84",
   "metadata": {},
   "source": [
    "**Linear Transformation, Eigen Values and Eigen vectors**\\\n",
    "Linear Transformation is transformation of the vector along the span with the help of basis vector. The transformation vectors are reperesented in the form of the matrix. Eigen vector is a such a vector of which the magnitude changes but the direction do not changes. The scaler values to tranform the eigen vectors are called the eigen values. In this equation $AX=\\lambda X$,$\\lambda$ is the eigen value which is the linear transfomration applied on X is equivalent to scaler multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee3116",
   "metadata": {},
   "source": [
    "**Eigendecompostion of a covarience matrix**\\\n",
    "Covarience matrix the spread and orientation of the data. \\\n",
    "Largest eigen vector of the covarience matrix always points in the direction of the largest varience of the data, and the magnitude of this vector equals the corresponding eigen value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475310f6",
   "metadata": {},
   "source": [
    "**Steps involved are**\n",
    "* center the data\n",
    "* find the covvarience matrix\n",
    "* find the eigen values\n",
    "* find the eigen vector corresponding to the eigen values\n",
    "* the one with the largest values give the first principal component and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f599d7da",
   "metadata": {},
   "source": [
    "**To transform the points we do**\\\n",
    "$U^{T}.X$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
