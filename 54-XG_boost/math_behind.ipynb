{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc1cafd",
   "metadata": {},
   "source": [
    "### **XGBoost Loss Function**\n",
    "$$\n",
    "L=\\sum_{i=1}^n L(Y_i,\\hat{Y_i})+\\Omega(f_2(X_i))\\\\\n",
    "\\Omega  (f)=\\gamma T+\\frac{1}{2} \\lambda||w||^2\\\\\n",
    "\\text{here T is the no of leaf nodes}\\\\\n",
    "\\gamma \\text{ is the regularization paramters}\\\\\n",
    "\\lambda \\text{ is also the regularization paramters}\n",
    "$$\n",
    "- We try to seek the output of the leaf nodes of the decision trees such that the loss function above is minimized.\n",
    "- We call the above function objective function rather than the loss function.\n",
    "- The first part in the objective function is the loss function.\n",
    "- The second part is the regularization function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f3260",
   "metadata": {},
   "source": [
    "- Consider we have a single model lets say $m_1$\n",
    "- Then the loss objective function becomes\n",
    "$$\n",
    "L^{(1)}=\\sum_{i=1}^nL(Y_i,\\hat{Y_i})\n",
    "$$\n",
    "- here there would be no second unction since the first model is only mean thus there would be no weight and the leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f21d8",
   "metadata": {},
   "source": [
    "- If we had the two models $m_1$ and $m_2$ then we would have the loss function :\n",
    "$$\n",
    "L^{(2)}=\\sum_{i=1}^n L(Y_i,f_1(X_i)+f_2(X_i))+\\Omega(f_2(X_i))\\\\\n",
    "\\hat{Y_i}=f_1(x_i)+f_2(x_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282d3273",
   "metadata": {},
   "source": [
    "- Now if we have three models $m_1$ , $m_2$ and $m_3$\n",
    "- The objective function is:\n",
    "$$\n",
    "L^{(3)}=\\sum_{i=1}^n (Y_i,f_1(X_i)+f_2(X_i)+f_3(X_i))+\\Omega(f_3(X_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43f203",
   "metadata": {},
   "source": [
    "- Now we have the T models\n",
    "- Then \n",
    "$$\n",
    "L^{(T)}=\\sum_{i=1}^n (Y_i,f_1(X_i)+f_2(X_i)+f_3(X_i)+...+f_T(X_i))+\\Omega(f_T(X_i))\n",
    "$$\n",
    "- This is the objective function at stage T.\n",
    "- At any stage T.\n",
    "$$\n",
    "L^{(T)}=\\sum_{i=1}^n L(Y_i,\\hat{Y_i}^{T-1}+f_T(X_i))+\\Omega f_T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0594e5",
   "metadata": {},
   "source": [
    "### **Taylor Series Expansion**\n",
    "- The above function is the optimization problem where we find the optimal value of the $f_T$.\n",
    "- Here we cannot do the optimization of the above objective function.\n",
    "- Since the curve obtained from the decision tree is not linear and non diffrentiable(The ouptut is piecewise constant).\n",
    "- Thus we cannot use :$\\frac{\\delta L}{\\delta b}$ like pratial differention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d000873",
   "metadata": {},
   "source": [
    "- The loss curve of the decision tree is not smooth. \n",
    "- Thus we need to somehow approximate loss function in the objective function i.e is approximate smooth loss curve.\n",
    "- Thus we use Tylor series in the approximation of the loss function.\n",
    "- Applying tylor series on the objective function upto second degree polynomial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01bb8f",
   "metadata": {},
   "source": [
    "$$\n",
    "L^{(t)}=\\sum_{i=1}^n L(Y_i,\\hat{Y_i}^{t-1}+f_t(X_i))+\\Omega (f_t(X_i))\\\\\n",
    "X\\rightarrow \\hat{Y_i}^{t-1}+f_t(X_i)\\\\\n",
    "a\\rightarrow \\hat{Y_i}^{t-1}\\\\\n",
    "\\text{Then}\\\\\n",
    "f(a)=\\sum_{i=1}^n L(Y_i,\\hat{Y_i}^{t-1})\\\\\n",
    "f^{'}(a)(x-a)= \\frac{\\delta f}{\\delta x}=\\sum_{i=1}^n\\frac{\\delta L(Y_i,\\hat{Y_i}^{t-1})}{\\delta \\hat{Y_i}^{t-1}}f_t(X_i)\\\\\n",
    "\\frac{f^{''}}{2}(x-a)^2=\\sum_{i=1}^n \\frac{\\delta ^2 L(Y_i,\\hat{Y_i}^{t-1})}{\\delta \\hat{y_i}^{t-1}} f_t ^2(X_i)\\\\\n",
    "\\text{The diffrentiation on fthe loss function is called gradient and is denoted by }g_i\\\\\n",
    "\\text{The double differentiation is called hesion }h_i\\\\\n",
    "L^{t}=\\sum_{i=1}^n [L(Y_i,\\hat{Y_i}^{(t-1)})+g_i f_t(X_i)+\\frac{1}{2} h_i f_t ^2(X_i)] +\\Omega(f_t(X_i))\\\\\n",
    "L^{(t)}=\\sum_{i=1}^n [g_if_t(X_i)+\\frac{1}{2} h_i f_t ^2(X_i)]+\\Omega(f_t(X_i))\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd07ad",
   "metadata": {},
   "source": [
    "- Now expanding the regualrization funxtion in the objective function.\n",
    "$$\n",
    "\\Omega(f_t(X_i))=\\gamma T+\\frac{1}{2} \\lambda [w_1^2+w_2^2+w_3^2+w_4^2+...+w_T^2]\\\\\n",
    "=\\gamma T+\\frac{1}{2} \\sum_{j=1}{T} w_j^2\\\\\n",
    "L^{(t)}=\\sum_{i=1}^T[\\sum_{i\\in \\pm j}g_iw_j+\\frac{1}{2}\\sum_{i\\in \\pm j}h_iw_j^2]+\\gamma T+\\frac{1}{2}\\sum_{i=1}^Tw_j^2\\\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01826886",
   "metadata": {},
   "source": [
    "The general objective function in XGBoost is:\n",
    "\n",
    "$$\n",
    "Obj = \\sum_{i=1}^n L(Y_i, \\hat{Y}_i) + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $L$ is the training loss,  \n",
    "- $\\Omega(f)$ is the regularization term.  \n",
    "\n",
    "The regularization is defined as:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_j^2\n",
    "$$\n",
    "\n",
    "- $T$: number of leaves  \n",
    "- $w_j$: weight of leaf $j$  \n",
    "- $\\gamma$: penalty for adding a new leaf  \n",
    "- $\\lambda$: L2 regularization term on weights  \n",
    "\n",
    "---\n",
    "\n",
    "### **Boosting Iterations**\n",
    "\n",
    "If we have built models up to iteration $t-1$, the prediction at iteration $t$ is:\n",
    "\n",
    "$$\n",
    "\\hat{Y}_i^{(t)} = \\hat{Y}_i^{(t-1)} + f_t(X_i)\n",
    "$$\n",
    "\n",
    "So the objective at iteration $t$ is:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} = \\sum_{i=1}^n L(Y_i, \\hat{Y}_i^{(t-1)} + f_t(X_i)) + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Second Order Taylor Expansion**\n",
    "\n",
    "To simplify optimization, expand the loss function using a 2nd order Taylor expansion around $\\hat{Y}_i^{(t-1)}$:\n",
    "\n",
    "$$\n",
    "L(Y_i, \\hat{Y}_i^{(t-1)} + f_t(X_i)) \\approx L(Y_i, \\hat{Y}_i^{(t-1)}) \n",
    "+ g_i f_t(X_i) + \\tfrac{1}{2} h_i f_t^2(X_i)\n",
    "$$\n",
    "\n",
    "where  \n",
    "- $ g_i = \\frac{\\partial L(Y_i, \\hat{Y}_i)}{\\partial \\hat{Y}_i} $ (gradient)  \n",
    "- $ h_i = \\frac{\\partial^2 L(Y_i, \\hat{Y}_i)}{\\partial \\hat{Y}_i^2} $ (hessian)  \n",
    "\n",
    "Dropping the constant term $L(Y_i, \\hat{Y}_i^{(t-1)})$, the objective becomes:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} \\approx \\sum_{i=1}^n \\Big[ g_i f_t(X_i) + \\tfrac{1}{2} h_i f_t^2(X_i) \\Big] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Tree Structure**\n",
    "\n",
    "Assume $f_t(X)$ is a decision tree with $T$ leaves.  \n",
    "Each leaf $j$ has a score $w_j$, and samples $X_i$ belong to exactly one leaf.  \n",
    "\n",
    "If $I_j$ is the set of instances in leaf $j$, then:\n",
    "\n",
    "$$\n",
    "f_t(X_i) = w_j \\quad \\text{if } X_i \\in I_j\n",
    "$$\n",
    "\n",
    "So the objective becomes:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} = \\sum_{j=1}^T \\Bigg[ \\Big(\\sum_{i \\in I_j} g_i \\Big) w_j \n",
    "+ \\frac{1}{2} \\Big(\\sum_{i \\in I_j} h_i + \\lambda \\Big) w_j^2 \\Bigg] + \\gamma T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimal Leaf Weights**\n",
    "\n",
    "To minimize with respect to $w_j$, take derivative and set to zero:\n",
    "\n",
    "$$\n",
    "w_j^* = - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "This is the **optimal output score for each leaf**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Similarity Score (Leaf Quality)**\n",
    "\n",
    "Plugging the optimal $w_j^*$ back into the objective gives the \"score\" of a tree:\n",
    "\n",
    "$$\n",
    "Obj^{(t)} = -\\frac{1}{2} \\sum_{j=1}^T \\frac{\\Big(\\sum_{i \\in I_j} g_i \\Big)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma T\n",
    "$$\n",
    "\n",
    "The term for a single leaf is called the **similarity score**:\n",
    "\n",
    "$$\n",
    "Score(I_j) = \\frac{\\Big(\\sum_{i \\in I_j} g_i \\Big)^2}{\\sum_{i \\in I_j} h_i + \\lambda}\n",
    "$$\n",
    "\n",
    "This score measures how “pure” or “good” a leaf is for reducing loss.  \n",
    "It is used in split decisions during tree construction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8bd64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
