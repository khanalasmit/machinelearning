{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d082fc",
   "metadata": {},
   "source": [
    "# üåü Kernel Trick in SVM\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Kernel Trick?\n",
    "- Support Vector Machines (SVMs) work well when data is **linearly separable**.  \n",
    "- For **non-linear datasets**, we can‚Äôt separate classes with a straight line (or hyperplane in higher dimensions).  \n",
    "- **Kernel Trick** allows us to implicitly map the data from a **lower-dimensional space** into a **higher-dimensional feature space** where it may become linearly separable.  \n",
    "- Importantly, we don‚Äôt compute this mapping explicitly ‚Äî instead, we use a **kernel function**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Kernel Function\n",
    "A kernel function is defined as:  \n",
    "\n",
    "$$\n",
    "K(x,y) = \\phi(x) \\cdot \\phi(y)\n",
    "$$\n",
    "\n",
    "where $\\phi(\\cdot)$ is the feature mapping to higher dimensions.  \n",
    "\n",
    "### Common example: **RBF (Radial Basis Function) kernel**\n",
    "$$\n",
    "K(x,y) = e^{-\\gamma \\lVert x-y \\rVert^2}\n",
    "$$\n",
    "\n",
    "- Here, $\\gamma > 0$ is a parameter that controls how fast similarity decreases with distance.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Role of $\\gamma$\n",
    "- **Small $\\gamma$:**\n",
    "  - Wider influence (points far apart can still be considered similar).  \n",
    "  - Decision boundary is smoother.  \n",
    "\n",
    "- **Large $\\gamma$:**\n",
    "  - Very localized influence (only close neighbors matter).  \n",
    "  - Decision boundary can become very wiggly (risk of overfitting).  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Infinite-Dimensional Mapping (Taylor Expansion Insight)\n",
    "\n",
    "Using the Taylor series expansion of the exponential function:  \n",
    "\n",
    "$$\n",
    "K(x,y) = e^{-\\gamma \\lVert x-y \\rVert^2} \n",
    "= 1 - \\gamma \\lVert x-y \\rVert^2 + \\frac{\\gamma^2 \\lVert x-y \\rVert^4}{2!} - \\frac{\\gamma^3 \\lVert x-y \\rVert^6}{3!} + \\cdots\n",
    "$$\n",
    "\n",
    "### Interpretation:\n",
    "- The first term corresponds to a **constant feature**.  \n",
    "- The second term corresponds to **linear features**.  \n",
    "- Higher-order terms correspond to **quadratic, cubic, ‚Ä¶ features**.  \n",
    "- Thus, the kernel effectively maps the data into an **infinite-dimensional space**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Important Clarification\n",
    "- ‚ùå **Incorrect:** ‚ÄúNever just use the kernel function directly.‚Äù  \n",
    "- ‚úÖ **Correct:** In SVM, we **always use the kernel function directly** ‚Äî that‚Äôs the whole point of the kernel trick!  \n",
    "\n",
    "We never explicitly compute $\\phi(x)$.  \n",
    "We only compute $K(x,y)$, which gives the inner product in the higher-dimensional space.  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Key Intuition\n",
    "- **Close points:**  \n",
    "  $$\n",
    "  K(x,y) \\approx 1 \\quad \\text{(high similarity)}\n",
    "  $$  \n",
    "\n",
    "- **Far points:**  \n",
    "  $$\n",
    "  K(x,y) \\approx 0 \\quad \\text{(low similarity)}\n",
    "  $$  \n",
    "\n",
    "- Kernel trick = solving a **non-linear problem** as if it were **linear in an unseen higher-dimensional space**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Takeaway\n",
    "The **kernel trick** allows SVM to create **non-linear decision boundaries** by implicitly mapping data to higher dimensions using kernel functions (like RBF).  \n",
    "You don‚Äôt compute the mapping ‚Äî you just compute the **kernel function**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85482d88",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
