{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e24b8c4",
   "metadata": {},
   "source": [
    "## **SUPPORT VECTOR MACHINES**\n",
    "- It imporves the idea of the logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131166d",
   "metadata": {},
   "source": [
    "### **Geometric intuition**\n",
    "- There can be multiple lines to classify the linearly classified lines.\n",
    "- So issue is which one to choose.\n",
    "- This issue is resolved by SVM.\n",
    "- It wants to classify points as widely as possible.\n",
    "- It is the imporvement over the Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de632f8c",
   "metadata": {},
   "source": [
    "**Core Logic Behind the selection of the HyperPlane**\n",
    "- We fist select the hyperplane.\n",
    "- Then we go in the positive in direction parrallel to the hyperplane.\n",
    "- We stop when we find the first green point.\n",
    "- This hyperplane is called $\\pi +$ hyperplane\n",
    "- Also simlarly in the negetive direction is called $\\pi -$.\n",
    "- We than calculate the distance between $\\pi + \\text{and} \\pi -$, this is called margin.\n",
    "- We try to maximize the margin. \n",
    "- The points touched by the $\\pi$ hyperplanes are called support vectors. \n",
    "- Support Vectors Machines are very robust to the outliers.\n",
    "- We can use for the non linear data with the help for kernel\n",
    "- Can be used for both classification and regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96664d",
   "metadata": {},
   "source": [
    "### **MATHEMATICS OF SUPPORT VECTOR MACHINES**\n",
    "A vector is made perpendicular to the hyper plane.\n",
    "- Lets say there is unknown point. Then we want to classify if it is 0 and 1. \n",
    "- Now the goal to define the classification of the vector(point). We make the projection of the vector over W(the perependicular vector). We thus calulated $\\vec{W}.\\vec{u}$.\n",
    "- The decsion rule is defined as\n",
    "$$\n",
    "\\vec{W}.\\vec{U} \\ge C \\text{ for the postive points.}\\\\\n",
    "\\vec{W}.\\vec{U}-C \\ge C\\\\\n",
    "\\vec{W}.\\vec{U}+B \\ge C\\\\\n",
    "\\text{Here W are the weights.}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eea6c2",
   "metadata": {},
   "source": [
    "- Now we need to find the value of w and b.\n",
    "$$\n",
    "\\text{The hyper plane is :}\\\\\n",
    "W^T.X+b=0\\\\\n",
    "W^T.X+b=1 \\text{for the }\\pi +\\\\\n",
    "W^T.X=b-1 \\text{for the }\\pi -\\\\\n",
    "\\text{These equations are the assumptions}\n",
    "$$\n",
    "- We require the hyper plane to be exacltly at the classification boundary. Thus we take 1 and -1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ec3ab",
   "metadata": {},
   "source": [
    "- When maximizing the distance d, there is constarint that the points cannot be above or below then postive or negetive hyperplanes. Thus the value of the point above the postive hyperplane should have $\\vec{W}.\\vec{U}+b \\ge 1$ and similar for the next one.\n",
    "$$\n",
    "Y_i(\\vec{W}.\\vec{X_i}+b) \\ge 1.\\\\\n",
    "\\text{for support vectors}.\\\\\n",
    "Y_i(\\vec{W}.X_i+b)=1\\\\\n",
    "\\text{Now for the value of d}\\\\\n",
    "$$\n",
    "![image](1.png)\\\n",
    "![image](2.png)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bace395",
   "metadata": {},
   "source": [
    "- This is called **Hard margin SVM**\n",
    "- That it this is used only for linearly separable.\n",
    "- Thus the extension of this called **soft margin SVM** is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcae335",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
