{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047656d6",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "Batch Gradient Descent is an optimization algorithm used to minimize the cost function in machine learning and deep learning models. In this method, the entire training dataset is used to compute the gradient of the loss function with respect to the model parameters. The parameters are then updated in the direction that reduces the loss.\n",
    "\n",
    "### Mathematical Description\n",
    "\n",
    "Given a cost function \\( J(\\theta) \\) over a dataset with \\( m \\) examples, the update rule for the parameters $( \\theta )$ is:\n",
    "\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $( \\alpha $) is the learning rate,\n",
    "- $( \\nabla_\\theta J(\\theta) $) is the gradient of the cost function with respect to \\( \\theta \\), computed over the entire dataset.\n",
    "\n",
    "For linear regression, the cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "and the update for each parameter \\( \\theta_j \\) is:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Stable Convergence:** Since the gradient is computed using the whole dataset, updates are more stable and less noisy.\n",
    "- **Deterministic:** For a given dataset and initial parameters, the updates are deterministic and reproducible.\n",
    "- **Efficient Vectorization:** Allows for efficient computation using matrix operations, leveraging optimized linear algebra libraries.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- **Computationally Expensive:** Processing the entire dataset for each update can be slow and resource-intensive, especially for large datasets.\n",
    "- **Memory Intensive:** Requires loading the entire dataset into memory, which may not be feasible for very large datasets.\n",
    "- **Slower Updates:** Model parameters are updated less frequently compared to stochastic or mini-batch gradient descent, potentially leading to slower convergence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
