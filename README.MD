## ðŸ“˜ Machine Learning from Beginning ##

| **SN** | **Work Done**                     | **Details**                                                                                                                                         |
|--------:|----------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|
| **1**   | Tensor Details                   | Learned about different types of tensors: 0D, 1D, 2D, etc.                                                                                           |
| **2**   | Working with CSV Files           | Explored CSV (Comma Separated Values) files using both **Pandas** and Python's built-in **`csv`** module.                                          |
| **3**   | Working with JSON Files          | Worked with JSON (JavaScript Object Notation) files: reading, writing, and parsing structured data.                                                |
| **4**   | Worked on API (Application Programming Interface) | Fetched data from APIs and formatted the responses into **DataFrames** using Pandas.                                                                |
| **5**   | Web Scraping and BeautifulSoup   | Used **Selenium** for web automation and scraping. Created a bot that:<br>1. Navigates to YouTube<br>2. Opens a specific channel<br>3. Scrolls down<br>4. Extracts all video descriptions into a DataFrame. |
| **6** | UNIVARIENT ANALYSIS | Done the univareint analysis in the **`advertisment.csv`** data |
| **7**| MULTIVARIENT ANALYSIS| Multivarient analysis on <br>1. **`flight dataset`** <br>2. **`tips dataset`** <br>3. **`iris dataset`**|
| **8**| FEATURE STANDRADIZATION | <br> 1. Introduction of the feature engineering <br> 2. Componentes of the feature engineering <br> 3. Detail of the feature scaling <br> 4. Feature Standradization <br> 5. Z-score noramalization of **`adverstisment.csv`** <br>6. Min max scaling |
|**9**| FEATURE ENCODING (CONTINUE OF FEATURE ENGINEERING)|<br> 1. Encoding and its defination <br> 2. Types of encoding <br>3. Ordinal Encoding <br>4. Label encoding <br> All the works were done on **`Student_performance_10k.csv`**| |
|**10**| ONEHOTENCODING| Deatails of the one hot encoding. OnehotEncoding using <br> 1.```pd.get_dummies()```<br>2. ```pd.get_dummies(dropfirst=True)``` <br>3. ```sklearn.preprocessing.OneHotEncoder()```|    
|**11**| COLUMN TRANSFOMER|A way of putting all the preprocessing tasks such as encoding, imputation and other tasks in a single transformer. code is like this:<br> ```from sklearn.compose import ```<br>```ColumnTransformertransformer=ColumnTransformer(transformers=[],reminder=(column to be left after the transfomation where we can mostly use the keyword 'passthrough' ))```<br>```transformer.fit_transform()```|
|**12**| SklearnPipelines| First fitted the decision tree classifier on the titanic dataset which had missing and the non numeric categorical variables. Thus the imputation was done then the encoding.<br> After that sklearn pipelines was used for the series of the imputation, encoding and fitting the model| 
|**13**| Function transfomer| Used the log tranformation on the data to see the impact of the transformation on distributiona and the machine learning models|
|**14**| Power transfomer| Used the box-cox and yeo-jhoson tranform which are used to make the distributions gaussian|
|**15**| BINNING AND BINARIZATION| Description and types of binning. Did the binning and binarization titatnic dataset but it is not the best example to work with so it is not fully reliable example|
|**16**| HANDELLING MIXED DATA| Columns with the numericaL and non numric values and a single data with both numbers and strings|
|**17**| HANDELLING DATES|```pd.to_datetime()``` can be used to convert objects to datetime which helps to extract the year, month , days and other extraction|
|**18**| HANDELLING MISSING DATAS|1. Description of the ways of handelling missing datas<br> 2. Handelling catergorical missing datas <br> 3.Using scikit learn for the imputation <br>4. Arbitaary Imputation <br>5. End of distribution imputation. |
|**19**| RANDOM IMPUTATION|Random impuation using pandas where missing values are imputed with randomly selected values from the data. Use of the missing indicator and evalution of its impact in the machinelearing model. |
|**20**| AUTOMATIC PARAMTER PARAMETERS FOR  IMPUTATION|Use of the gird search cross validation for the best imputing startegies. |
|**21**| MULTIVARINET IMPUTATION|1. Use of the knn imputer for the miultivareint impuatation with the detail and code example of the knn impuatation.<br>2. Iterative imputation and its detail in the ```iterative_impuatation.ipynb``` notebook |
|**22**| OUTLIERS DETECTION|Defination of outliers detection and description of the outliers' impact for machinlearning algorithm. Techniques are to be discussed in other files with the code example. |
|**23(22)**| Z-SCORE METHOD|Defination and the descreption of the z-score method for the outliers detection. Use of the z-score method to find the outliers|
|**24(22)**| IQR METHOD|Defination and the descreption of the IQR method for the outliers detection. Use of the trimming and capping  method to remove the outliers|
|**25(22)**| PERCENTILE METHOD|Defination and the descreption of the percentile method for the outliers detection. Use of the **trimming** and **winsorization**  method to remove the outliers|
|**26**| DENSITY CLUSTERING|Use of the **DBSCAN** algorithm for the density based clustering. <br>1.Use of the sklearn datasets for the density clustering:<br> ```sklearn.clustering.DBSCAN(epsilon,min_samples)``` <br>2. Implementation of the DBSCAN without using sklearn methods.|
|**27**| FEATURE CONSTRUCTION|Constructing the new features from the data by combing the features and splitting the features known as feature splitting.|
|**28**| CURSE OF DIMENSIONALITY|Description of curse of dimesinality<br>1. Discussion on what happens when dimesnionality increases|
|**29**| PCA|Description of the pca<br>-defination of pca -advantages -wroking mechanism -explaination of the math involve -desctiption of eigen vectors -description of eigen values -desctiption of linear algebra -desription of covarience matrix -pca to transform ```MINSET``` data |
|**30**|SIMPLE LINEAR REGRESSION|Description of linear regression,<br>- Mathematical model of the linear regression <br> - intution code on simple linear regression <br> - code from scratch to make a liner regression model |
|**31**| REGRESSION METRICS|- Desciption of the regression metrics <br> -description of mean abosolute error, mean squared error , root mean squared error,r2 score,adjusted r2 score <br> - Advanteages and disadvantages of each model <br> -description of adjusted r2 score with  |
|**32**| MULTIPLE LINEAR REGRESSION|- Descriptionof multiple liner regresion <br> - code for intution <br> - mathematical derivation of the multiple linear regression <br> - code from scartch using the mathematical formula <br> - comparsion of the sklearn model and the self built model|
|**33**| GRADIENT DESCENT (BATCH GRADIENT DESCENT)|- Introduction to gradient descent as an optimization algorithm for minimizing loss functions.<br>- Mathematical formulation of batch gradient descent.<br>- Implementation of batch gradient descent from scratch for linear regression.<br>- Comparison of custom implementation with scikit-learn's LinearRegression.<br>- Visualization of the loss surface and the effect of weights and bias.<br>- Discussion of the advantages and limitations of batch gradient descent (computational cost, memory usage, infrequent updates).|
|**34**| TYPES OF GRADIENT DESCENT (STOCHASTIC & MINI-BATCH)|- Explanation of different types of gradient descent: batch, stochastic, and mini-batch.<br>- Detailed markdown on the problems with batch gradient descent and the motivation for stochastic gradient descent (SGD).<br>- Mathematical formulation of SGD using LaTeX.<br>- Implementation of stochastic gradient descent from scratch for linear regression.<br>- Comparison of SGD with batch gradient descent and scikit-learn's SGDRegressor.<br>- Discussion of the advantages (faster updates, scalability, ability to escape local minima) and disadvantages (noisy updates, possible oscillation, sensitivity to learning rate) of SGD.<br>- Brief mention of mini-batch gradient descent and its trade-offs.<br>- Visualization of the loss surface and the effect of different update strategies.|
|**35**| POLYNOMIAL REGRESSION | - Introduction to polynomial regression and its motivation for modeling nonlinear relationships.<br>- Mathematical formulation for both univariate and multivariate polynomial regression using LaTeX.<br>- Implementation of polynomial regression using scikit-learn's `PolynomialFeatures` and `LinearRegression`.<br>- Code for fitting and visualizing polynomial regression curves of various degrees.<br>- Comparison of linear and polynomial regression performance using $R^2$ score.<br>- 3D polynomial regression example with visualization using Plotly.<br>- Discussion of overfitting and underfitting in polynomial regression.<br>- Exploration of the effect of polynomial degree on model complexity and fit.|
|**36**| BIAS-VARIANCE TRADEOFF | - Explanation of the bias-variance tradeoff in machine learning.<br>- Demonstration of how model complexity affects bias, variance, and loss.<br>- Visualization of bias, variance, and loss as a function of model parameters.<br>- Code examples and plots in `bias_varience_tradeoff.ipynb`.|
|**37**| RIDGE REGRESSION | - Introduction to Ridge Regression and its mathematical formulation.<br>- Implementation from scratch and using scikit-learn.<br>- Explanation of the effect of the regularization parameter $\lambda$ (alpha) on coefficients.<br>- Gradient descent approach for Ridge Regression.<br>- Ridge regression for multi-dimensional data.<br>- Bias-variance tradeoff visualization for Ridge Regression.<br>- All code and explanations in the `37-Ridge_regresssion` folder.|
|**38**| LASSO REGRESSION | - Introduction to Lasso Regression and its sparsity property.<br>- Mathematical derivation of the Lasso solution.<br>- Implementation and intuition behind Lasso.<br>- Key points and visualizations showing how Lasso drives coefficients to zero.<br>- All code and explanations in the `38-lasso_regression` folder.|
|**39**| LOGISTIC REGRESSION & PERCEPTRON | - Implementation of the Perceptron algorithm from scratch.<br>- Visualization and animation of the decision boundary during training.<br>- Comparison with scikit-learn's Logistic Regression.<br>- Code for both Perceptron and Logistic Regression in `40-Logistic_Regression/perceptron_code.ipynb`.<br>- Animation of the perceptron learning process (`animation.gif`).|
<br>
Note:pickle notebook is to open the models that are exported using the pickle
